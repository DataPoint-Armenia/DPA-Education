---
title: "Gary's Regression Example"
output: html_document
---

## Regression & Classification 

For this one we are going to use our friendly iris dataset

```{r}
library(normtest)
require("datasets")
data("iris")
str(iris)
head(iris)

require(GGally)
g1 <- ggpairs(data=iris, title="tips data",
  mapping=ggplot2::aes(colour = Species),
  lower=list(combo=wrap("facethist",binwidth=1)))
g1
```

A picture is worth a thousand words, what are your takings from the visualization above?

### Linear Regression

In statistics and in machine learning, you sometimes are interested in making predictions with variables. You want to know how variable X is able predict Y. In stats terms, you are regressing y on x. 

Take a look at the visualization below, this plots the Petal Length against the Width, do you think there is any pattern? Any linear relationship?

```{r}
ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) + geom_point() + ggtitle("Petal Length vs Petal Width")
```

Something noticed was that Petal Length and Petal Width a=have a really high correlation of 96.3%. This shows a strong linear relationship. That is great! You can run the most simple case of linear regression in statistics. This is where you draw a line of best fit onto your dataset. 

This is drawn on the whole dataset. What do you think do you think the line would be a good fit? 

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(aes(color = Species)) +
  geom_smooth(method = "lm") + ggtitle("Petal Length vs Petal Width")
```

You can be even more modular and try to draw based on the different species of iris plants

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point() +
  geom_smooth(method = "lm") + ggtitle("Petal Length vs Petal Width")
```

For now, we are just going to ignore the groups when running a basic linear regression. 

```{r}
lm1 <- lm(Petal.Width ~ Petal.Length, data = iris)
summary(lm1)
stargazer(lm1)
```

So the way that this is modeled is:

$$\hat{y} = intercept \ + B_1X_1  $$
$$Predicted \  Pedal \ Width = -0.363076 \ + 0.415755X_1  $$
Going back to the original visualization 

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(aes(color = Species)) +
  geom_smooth(method = "lm") + ggtitle("Petal Length vs Petal Width")
```


$$Predicted \  Pedal \ Width = -0.363076 \ + 0.415755X_1  $$

So let's try to interpret this.  What is the main thing that can be changed from the graph on the X side. The Petal Length, so try increasing the X1 by one point what would be the new equation? Remember the model predicts an average value. Let's try with X = 1,2,3

$$0.052679 = -0.363076 \ + 0.415755*1  $$

$$0.468434 = -0.363076 \ + 0.415755*2  $$

$$0.884189 = -0.363076 \ + 0.415755*3  $$

So in each case, as your X variable changes, you would expect your y variable to change by B1 amount (In this case 0.415755)

Linear regressions are really good when your Y variable is continous meaning the numbers can go from -infinity to infinity. While your X's can be continous or discrete/categorical. When the variable that you want to predict is with respect to categories, you would want to use a logistic regression. 

### Logistic Regression

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point() 
```

Remember back to this graph, say instead of drawing a linear line that best predicts for petal width. Try to see if you can use a linear line to seperate out the between the different species. 

Try it out for:
- setosa vs versicolor, 
- setosa vs virginica, 
- versicolor vs verginica. 

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point()  + geom_hline(yintercept=.75)
```

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point()  + geom_hline(yintercept=1.6)
```


```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point()  + geom_vline(xintercept=2.3)
```

```{r}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point()  + geom_vline(xintercept=5)
```

It might be easier to boil down to 1 axis. 

```{r}
ggplot(iris, aes(x=Petal.Length, color=Species)) +
  geom_histogram(fill="white")
```

```{r}
ggplot(iris, aes(x=Petal.Length, color=Species)) +
  geom_histogram(fill="white") + geom_vline(xintercept=2.3)
```


```{r}
library(dplyr)
iris2 <- iris %>% filter(Species != "virginica")
iris2$Species <- droplevels(iris2$Species)
iris2$versicolor <- ifelse(iris2$Species == "versicolor", 1, 0) 

```

```{r}
ggplot(iris2, aes(x=Petal.Length, y=versicolor)) +
  geom_point() +
  geom_smooth(method = "lm", 
    method.args = list(family = "binomial"), 
    se =FALSE) + ggtitle("Predicting Category with Linear Regression")
```


```{r}
ggplot(iris2, aes(x=Petal.Length, y=versicolor)) +
  geom_point() +
  geom_smooth(method = "glm", 
    method.args = list(family = "binomial"), 
    se = FALSE) + ggtitle("Predicting Category with Logistic Regression")
```

The above logistic regression model is much more able to follow the dots than the linear regression above. This new model is better but it also has a slighlty different interpretation. The sigmoid function is written as

$$f(x) = \frac{1}{1+e^{-x}}$$

```{r}
sigmoid = function(x) {
   1 / (1 + exp(-x))
}
x <- seq(-5, 5, 0.01)
plot(x, sigmoid(x), col='blue', title = "Sigmoid Function")

```
In this case, what we are doing is getting your results from the logistic regression model and fitting it into this model. 


```{r}
lm2 <- lm(versicolor ~ Petal.Length, data = iris2)
summary(lm2)
stargazer(lm2)
```

You see how the values differ. 

```{r}
logisticmodel <- glm(versicolor ~ Petal.Length, family = "binomial", data = iris2)
summary(logisticmodel)
stargazer(logisticmodel)
```

#### Sigmoid Equation

$$f(x) = \frac{1}{1+e^{-x}}$$

##### Linear Regression Model

$$f(x) = -46.2 + .336*Petal.Length$$
##### Logistic 

$$f(x) = \frac{1}{1+e^{-(-46.2 + .336*Petal.Length)}}$$
After some mathematical manipulation you would get 

$$p(x) = \frac{e^{(-46.2 + .336*Petal.Length)}}{1+e^{(-46.2 + .336*Petal.Length)}}$$


The mathematical explanation of this is really out of the scope of the original talk...

long story short, your equation below is an interpretation of log odds when it comes to probability. 

This means that for a one-unit increase in Petal Length you expect a 39.07 increase in the log-odds that the iris plant is versicolor, holding all other independent variables constant.


```{r}
summary(logisticmodel)
```

if you want to make it even more interpretable, you would exponenitate the results and you would end up with the odds interpretation. As the petal length increases by a factor of 1, you would expect the odds of your flower being veriscolor to increase by exp(39.07) (which is alot). This is pretty true considering how well the logistic regression model holds. 
