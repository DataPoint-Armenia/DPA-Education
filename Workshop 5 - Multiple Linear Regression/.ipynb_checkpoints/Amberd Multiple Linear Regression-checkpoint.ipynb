{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `amberd.jpg'\r\n",
      "/bin/sh: -c: line 0: `[](amberd.jpg)'\r\n"
     ]
    }
   ],
   "source": [
    "![](amberd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "\n",
    "- Bias Variance TradeOff\n",
    "\n",
    "- Statistical Approach\n",
    "Assumptions\n",
    "Closed Form Solution (From Hand)\n",
    "what happens when assumptions are not met\n",
    "How to make them meet those assumptions\n",
    "Pitfalls\n",
    "Multiple Comparison Bias\n",
    "\n",
    "- Interpretation Approach\n",
    "- Variable Interpretations and use case\n",
    "- BLUE\n",
    "- Interpreation \n",
    "- Polynomial Regression\n",
    "- Interaction Effect\n",
    "\n",
    "\n",
    "- Optimizing\n",
    "\n",
    "gradient descent. \n",
    "Use closed form Solution. Try making your own regression. \n",
    "\n",
    "- CS Approach\n",
    "Stepwise\n",
    "Regularization\n",
    "Lasso, Ridge\n",
    "\n",
    "\n",
    "- Metrics and subset regression\n",
    "\n",
    "Next Steps:\n",
    "    - Lesson: Always start off by doing simple. Then make more complex. Try to focus on \n",
    "    - This model is actually the first building block step into more complicated models such as:\n",
    "        - Stepwise Regression\n",
    "        - Lasso and Ridge Regression\n",
    "        - Logistic Regression\n",
    "        - Neural Networks\n",
    "        - Regression Splines\n",
    "        - Poisson Regression\n",
    "        - Time Series Modeling\n",
    "        - Instrumental Variables\n",
    "        - Panel Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Econometric Modeling\n",
    "\n",
    "\n",
    "All models are wrong. You don't know what is actually goiing on but its there to help simplify things more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Workshop\n",
    "\n",
    "In the previous workshop, Armen discussed about the Single Linear Regression.  \n",
    "- Use Case: \n",
    "    - Y Ouput: Numeric and Continuous \n",
    "        - Continous: Can go infinitely small, technically no limit and expand to cover any range of numbers.\n",
    "    - X Variables: Numeric or Factor (Continous or Discrete)\n",
    "        - Discrete: When variables can only take on certain numbers or levels. \n",
    "        - You cannot divide the number \n",
    "       \n",
    "- Optimization Function:\n",
    "    - Optimizing for MSE \n",
    "        Data Sciencec boils down to a metric or KPI \n",
    "\n",
    "- Interpretation:\n",
    "\n",
    "\n",
    "Pros and Cons:\n",
    "\n",
    "Parametric Modeling:\n",
    "\n",
    "Meaning the model does really well if all of its parameters are satisfied. \n",
    "\n",
    "What happens when they are not. How does it affect your coefficients, model accuracy?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Talking about models\n",
    "\n",
    "What is a model. Ultimately a simplification of the real world. \n",
    "\n",
    "Machine Learning:\n",
    "- Paremetric\n",
    "- NonParametric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distribution\n",
    "\n",
    "You have two random variables (distributions). I call them random because you have values that are distributed \n",
    "randomly around some point. it can take on any values. You want to try to quantify and predict with more confidence than randomly. \n",
    "\n",
    "This is where you try to use some outside information or variable to see if you can be better at predicting and identifying than before.\n",
    "\n",
    "\n",
    "For example, you have this distribution and it can take on any of these values. \n",
    "\n",
    "You can examine it through visualization and summary statistics. Once you have a better picture of its properties, \n",
    "understanding what it means, modality, what distribution falls under, you \n",
    "\n",
    "Say, I'm tyring to make a prediction. What do you think what be the most likely value? (Mean)\n",
    "So a big part of data Science is focusing on KPI (key performance indictators). \n",
    "Is there a way that we can quantify this? \n",
    "\n",
    "So we can do MSE, Mean Absolute Error, etc. \n",
    "But for simplicity sake let's pick MSE. \n",
    "\n",
    "\n",
    "Try picking just one number and minimizing the MSE. \n",
    "Exercise.\n",
    "\n",
    "Gradient descent is an efficient optimization algorithm that attempts to find a local or global minimum of the cost function\n",
    "(see photo)\n",
    "\n",
    "A local minimum is a point where our function is lower than all neighboring points. It is not possible to decrease the value of the cost function by making infinitesimal steps.\n",
    "A global minimum is a point that obtains the absolute lowest value of our function, but global minima are difficult to compute in practice.\n",
    "\n",
    "You can do this via gradient descent. X can take on any value, but there is one value that would result in best. \n",
    "Kind of looking like a convex functiion here. So this goes back to calculus. \n",
    "How do you find \n",
    "\n",
    "(y - y_bar)^2\n",
    "\n",
    "Literally show a convext graph using matplotlib with the value from 0 to 100 and loss function. \n",
    "\n",
    "\n",
    "If this is your formula. The derivative would be 1/n()-2b(y-b)\n",
    "\n",
    "The size of each step is determined by the parameter Î± (alpha), which is called the learning rate.\n",
    "The learning rate determines the size of the steps that are taken by the gradient descent algorithm\n",
    "\n",
    "TO DO for audience. \n",
    "Try to create yoour own\n",
    "\n",
    "import numpy\n",
    "\n",
    "x = [1, 1, 2, 3, 4, 3, 4, 6, 4]\n",
    "y = [2, 1, 0.5, 1, 3, 3, 2, 5, 4]\n",
    "\n",
    "\n",
    "def mean_square_error(df, y_hat):\n",
    "    '''\n",
    "    MSE = 1/n((y - y_hat)^2)\n",
    "    '''\n",
    "    y = numpy.array(df)\n",
    "    error = y-y_hat\n",
    "    squared_error = error**2\n",
    "    mean_squared_error_value = squared_error/len(y)\n",
    "    return(mean_squared_error_value)\n",
    "    \n",
    "    \n",
    "\n",
    "def gradient_descent(X, y, lr=0.05, epoch=10):\n",
    "    \n",
    "    '''\n",
    "    Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    m, b = 0.33, 0.48 # parameters\n",
    "    log, mse = [], [] # lists to store learning process\n",
    "    N = len(X) # number of samples\n",
    "    \n",
    "    for _ in range(epoch):\n",
    "                \n",
    "        f = y - (m*X + b)\n",
    "    \n",
    "        # Updating m and b\n",
    "        m -= lr * (-2 * X.dot(f).sum() / N)\n",
    "        b -= lr * (-2 * f.sum() / N)\n",
    "        \n",
    "        log.append((m, b))\n",
    "        mse.append(mean_squared_error(y, (m*X + b)))        \n",
    "    \n",
    "    return m, b, log, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 1, 2, 3, 4, 3, 4, 6, 4]\n",
    "y = [2, 1, 0.5, 1, 3, 3, 2, 5, 4]\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(df, y_hat):\n",
    "    '''\n",
    "    MSE = 1/n((y - y_hat)^2)\n",
    "    '''\n",
    "    y = np.array(df)\n",
    "    error = y-y_hat\n",
    "    squared_error = error**2\n",
    "    mean_squared_error_value = squared_error/len(y)\n",
    "    return(mean_squared_error_value)\n",
    "    \n",
    "    \n",
    "\n",
    "def gradient_descent0(y, lr=0.05, epoch=100):\n",
    "    \n",
    "    '''\n",
    "    Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    b = 0.48 # parameters\n",
    "    log, mse = [], [] # lists to store learning process\n",
    "    N = len(y) # number of samples\n",
    "    y = np.array(y)\n",
    "    \n",
    "    for _ in range(epoch):\n",
    "                \n",
    "        f = y - (b)\n",
    "    \n",
    "        # Updating m and b\n",
    "        b -= lr * (-2 * f.sum() / N)\n",
    "        \n",
    "        log.append((b))\n",
    "        mse.append(mean_squared_error(y, (b)))        \n",
    "    \n",
    "    return b, log, mse\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, lr=0.05, epoch=10):\n",
    "    \n",
    "    '''\n",
    "    Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    m, b = 0.33, 0.48 # parameters\n",
    "    log, mse = [], [] # lists to store learning process\n",
    "    N = len(X) # number of samples\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    for _ in range(epoch):\n",
    "                \n",
    "        f = y - (m*X + b)\n",
    "    \n",
    "        # Updating m and b\n",
    "        m -= lr * (-2 * X.dot(f).sum() / N)\n",
    "        b -= lr * (-2 * f.sum() / N)\n",
    "        \n",
    "        log.append((m, b))\n",
    "        mse.append(mean_squared_error(y, (m*X + b)))        \n",
    "    \n",
    "    return m, b, log, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.388888888888889"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3888381861296795,\n",
       " [0.6708888888888889,\n",
       "  0.8426888888888888,\n",
       "  0.9973088888888888,\n",
       "  1.1364668888888887,\n",
       "  1.2617090888888887,\n",
       "  1.3744270688888887,\n",
       "  1.4758732508888888,\n",
       "  1.5671748146888889,\n",
       "  1.6493462221088888,\n",
       "  1.7233004887868888,\n",
       "  1.7898593287970888,\n",
       "  1.8497622848062687,\n",
       "  1.9036749452145307,\n",
       "  1.9521963395819666,\n",
       "  1.9958655945126589,\n",
       "  2.035167923950282,\n",
       "  2.0705400204441426,\n",
       "  2.1023749072886173,\n",
       "  2.1310263054486445,\n",
       "  2.156812563792669,\n",
       "  2.180020196302291,\n",
       "  2.200907065560951,\n",
       "  2.2197052478937445,\n",
       "  2.236623611993259,\n",
       "  2.251850139682822,\n",
       "  2.2655540146034285,\n",
       "  2.2778875020319744,\n",
       "  2.2889876407176657,\n",
       "  2.298977765534788,\n",
       "  2.307968877870198,\n",
       "  2.3160608789720674,\n",
       "  2.3233436799637497,\n",
       "  2.3298982008562636,\n",
       "  2.335797269659526,\n",
       "  2.3411064315824626,\n",
       "  2.345884677313105,\n",
       "  2.3501850984706834,\n",
       "  2.354055477512504,\n",
       "  2.3575388186501427,\n",
       "  2.3606738256740174,\n",
       "  2.3634953319955048,\n",
       "  2.3660346876848433,\n",
       "  2.368320107805248,\n",
       "  2.3703769859136123,\n",
       "  2.37222817621114,\n",
       "  2.373894247478915,\n",
       "  2.3753937116199126,\n",
       "  2.3767432293468103,\n",
       "  2.3779577953010183,\n",
       "  2.379050904659805,\n",
       "  2.3800347030827136,\n",
       "  2.380920121663331,\n",
       "  2.381716998385887,\n",
       "  2.382434187436187,\n",
       "  2.3830796575814572,\n",
       "  2.3836605807122004,\n",
       "  2.3841834115298695,\n",
       "  2.3846539592657714,\n",
       "  2.3850774522280833,\n",
       "  2.385458595894164,\n",
       "  2.3858016251936367,\n",
       "  2.386110351563162,\n",
       "  2.386388205295735,\n",
       "  2.38663827365505,\n",
       "  2.3868633351784343,\n",
       "  2.3870658905494797,\n",
       "  2.3872481903834206,\n",
       "  2.3874122602339676,\n",
       "  2.3875599230994595,\n",
       "  2.3876928196784024,\n",
       "  2.387812426599451,\n",
       "  2.387920072828395,\n",
       "  2.3880169544344447,\n",
       "  2.388104147879889,\n",
       "  2.388182621980789,\n",
       "  2.388253248671599,\n",
       "  2.388316812693328,\n",
       "  2.388374020312884,\n",
       "  2.3884255071704845,\n",
       "  2.388471845342325,\n",
       "  2.3885135496969814,\n",
       "  2.3885510836161723,\n",
       "  2.388584864143444,\n",
       "  2.3886152666179887,\n",
       "  2.3886426288450786,\n",
       "  2.3886672548494596,\n",
       "  2.3886894182534024,\n",
       "  2.388709365316951,\n",
       "  2.3887273176741446,\n",
       "  2.388743474795619,\n",
       "  2.388758016204946,\n",
       "  2.3887711034733403,\n",
       "  2.3887828820148953,\n",
       "  2.3887934827022947,\n",
       "  2.3888030233209543,\n",
       "  2.388811609877748,\n",
       "  2.388819337778862,\n",
       "  2.3888262928898647,\n",
       "  2.388832552489767,\n",
       "  2.3888381861296795],\n",
       " [array([0.19628182, 0.0120349 , 0.00324478, 0.0120349 , 0.60275095,\n",
       "         0.60275095, 0.19628182, 2.08235589, 1.23144231]),\n",
       "  array([0.14881878, 0.00274964, 0.01304841, 0.00274964, 0.51711014,\n",
       "         0.51711014, 0.14881878, 1.92035952, 1.10762372]),\n",
       "  array([1.11709940e-01, 8.04675446e-07, 2.74795701e-02, 8.04675446e-07,\n",
       "         4.45641299e-01, 4.45641299e-01, 1.11709940e-01, 1.78017068e+00,\n",
       "         1.00179488e+00]),\n",
       "  array([0.08285438, 0.00206925, 0.04501001, 0.00206925, 0.38586174,\n",
       "         0.38586174, 0.08285438, 1.65854312, 0.91109132]),\n",
       "  array([0.06056372, 0.00761018, 0.06446675, 0.00761018, 0.33573948,\n",
       "         0.33573948, 0.06056372, 1.55275766, 0.83313746]),\n",
       "  array([0.04348239, 0.01557729, 0.08495808, 0.01557729, 0.29360971,\n",
       "         0.29360971, 0.04348239, 1.46053101, 0.76595925]),\n",
       "  array([0.03052321, 0.02516171, 0.10581429, 0.02516171, 0.25810693,\n",
       "         0.25810693, 0.03052321, 1.37994104, 0.70791287]),\n",
       "  array([0.02081529, 0.03574303, 0.12654023, 0.03574303, 0.22810978,\n",
       "         0.22810978, 0.02081529, 1.30936542, 0.65762649]),\n",
       "  array([0.01366201, 0.04685006, 0.14677742, 0.04685006, 0.20269618,\n",
       "         0.20269618, 0.01366201, 1.24743119, 0.61395258]),\n",
       "  array([0.00850696, 0.05812929, 0.16627379, 0.05812929, 0.18110685,\n",
       "         0.18110685, 0.00850696, 1.1929733 , 0.57592896]),\n",
       "  array([0.00490657, 0.06931975, 0.18485968, 0.06931975, 0.1627156 ,\n",
       "         0.1627156 , 0.00490657, 1.14500035, 0.54274687]),\n",
       "  array([0.00250793, 0.08023288, 0.20242869, 0.08023288, 0.1470052 ,\n",
       "         0.1470052 , 0.00250793, 1.10266641, 0.51372469]),\n",
       "  array([1.03094624e-03, 9.07364896e-02, 2.18922595e-01, 9.07364896e-02,\n",
       "         1.33547625e-01, 1.33547625e-01, 1.03094624e-03, 1.06524765e+00,\n",
       "         4.88286526e-01]),\n",
       "  array([2.53909994e-04, 1.00741985e-01, 2.34319357e-01, 1.00741985e-01,\n",
       "         1.21988057e-01, 1.21988057e-01, 2.53909994e-04, 1.03212302e+00,\n",
       "         4.65944426e-01]),\n",
       "  array([1.89925653e-06, 1.10194254e-01, 2.48623764e-01, 1.10194254e-01,\n",
       "         1.12031767e-01, 1.12031767e-01, 1.89925653e-06, 1.00275817e+00,\n",
       "         4.46283857e-01]),\n",
       "  array([1.37420319e-04, 1.19063626e-01, 2.61860062e-01, 1.19063626e-01,\n",
       "         1.03433437e-01, 1.03433437e-01, 1.37420319e-04, 9.76692138e-01,\n",
       "         4.28951676e-01]),\n",
       "  array([5.52877165e-04, 1.27339548e-01, 2.74066217e-01, 1.27339548e-01,\n",
       "         9.59884282e-02, 9.59884282e-02, 5.52877165e-04, 9.53526197e-01,\n",
       "         4.13646201e-01]),\n",
       "  array([0.00116451, 0.1350256 , 0.28528948, 0.1350256 , 0.08952565,\n",
       "         0.08952565, 0.00116451, 0.93291458, 0.400109  ]),\n",
       "  array([0.00190754, 0.14213561, 0.29558298, 0.14213561, 0.0839017 ,\n",
       "         0.0839017 , 0.00190754, 0.91455667, 0.38811807]),\n",
       "  array([0.00273224, 0.14869059, 0.3050031 , 0.14869059, 0.07899612,\n",
       "         0.07899612, 0.00273224, 0.89819053, 0.37748221]),\n",
       "  array([0.00360081, 0.15471641, 0.31360754, 0.15471641, 0.07470743,\n",
       "         0.07470743, 0.00360081, 0.88358734, 0.36803628]),\n",
       "  array([0.00448485, 0.16024198, 0.32145387, 0.16024198, 0.07094995,\n",
       "         0.07094995, 0.00448485, 0.87054681, 0.35963727]),\n",
       "  array([0.00536338, 0.16529788, 0.32859846, 0.16529788, 0.0676511 ,\n",
       "         0.0676511 , 0.00536338, 0.85889321, 0.35216104]),\n",
       "  array([0.00622119, 0.16991533, 0.33509573, 0.16991533, 0.06474928,\n",
       "         0.06474928, 0.00622119, 0.84847212, 0.34549959]),\n",
       "  array([0.00704761, 0.17412542, 0.34099766, 0.17412542, 0.06219202,\n",
       "         0.06219202, 0.00704761, 0.83914752, 0.33955866]),\n",
       "  array([0.00783544, 0.17795855, 0.34635344, 0.17795855, 0.05993455,\n",
       "         0.05993455, 0.00783544, 0.83079943, 0.33425588]),\n",
       "  array([0.00858016, 0.18144405, 0.35120933, 0.18144405, 0.0579385 ,\n",
       "         0.0579385 , 0.00858016, 0.82332183, 0.32951905]),\n",
       "  array([0.00927932, 0.1846099 , 0.35560853, 0.1846099 , 0.05617095,\n",
       "         0.05617095, 0.00927932, 0.81662089, 0.32528481]),\n",
       "  array([0.00993197, 0.18748258, 0.35959122, 0.18748258, 0.05460357,\n",
       "         0.05460357, 0.00993197, 0.81061346, 0.3214974 ]),\n",
       "  array([0.01053831, 0.19008695, 0.36319461, 0.19008695, 0.0532119 ,\n",
       "         0.0532119 , 0.01053831, 0.80522573, 0.3181077 ]),\n",
       "  array([0.01109939, 0.19244625, 0.36645301, 0.19244625, 0.05197475,\n",
       "         0.05197475, 0.01109939, 0.80039213, 0.31507233]),\n",
       "  array([0.01161679, 0.19458206, 0.36939802, 0.19458206, 0.05087375,\n",
       "         0.05087375, 0.01161679, 0.79605434, 0.31235294]),\n",
       "  array([0.01209254, 0.19651436, 0.3720586 , 0.19651436, 0.04989294,\n",
       "         0.04989294, 0.01209254, 0.7921604 , 0.30991556]),\n",
       "  array([0.01252887, 0.19826159, 0.37446129, 0.19826159, 0.04901836,\n",
       "         0.04901836, 0.01252887, 0.78866402, 0.30773008]),\n",
       "  array([0.01292818, 0.19984072, 0.37663032, 0.19984072, 0.04823786,\n",
       "         0.04823786, 0.01292818, 0.78552389, 0.30576976]),\n",
       "  array([0.01329291, 0.20126728, 0.3785878 , 0.20126728, 0.04754076,\n",
       "         0.04754076, 0.01329291, 0.78270313, 0.30401083]),\n",
       "  array([0.01362551, 0.20255553, 0.38035388, 0.20255553, 0.04691771,\n",
       "         0.04691771, 0.01362551, 0.78016878, 0.30243213]),\n",
       "  array([0.01392836, 0.20371847, 0.38194686, 0.20371847, 0.04636048,\n",
       "         0.04636048, 0.01392836, 0.77789138, 0.30101482]),\n",
       "  array([0.01420378, 0.20476796, 0.38338338, 0.20476796, 0.04586182,\n",
       "         0.04586182, 0.01420378, 0.77584457, 0.29974208]),\n",
       "  array([0.01445396, 0.20571481, 0.38467857, 0.20571481, 0.04541533,\n",
       "         0.04541533, 0.01445396, 0.77400474, 0.29859892]),\n",
       "  array([0.01468098, 0.20656884, 0.38584609, 0.20656884, 0.04501535,\n",
       "         0.04501535, 0.01468098, 0.77235076, 0.29757195]),\n",
       "  array([0.01488682, 0.20733897, 0.38689838, 0.20733897, 0.04465689,\n",
       "         0.04465689, 0.01488682, 0.7708637 , 0.29664918]),\n",
       "  array([0.0150733 , 0.20803332, 0.38784667, 0.20803332, 0.0443355 ,\n",
       "         0.0443355 , 0.0150733 , 0.76952656, 0.29581992]),\n",
       "  array([0.01524212, 0.20865923, 0.38870112, 0.20865923, 0.04404724,\n",
       "         0.04404724, 0.01524212, 0.76832413, 0.29507457]),\n",
       "  array([0.01539487, 0.20922335, 0.38947093, 0.20922335, 0.04378861,\n",
       "         0.04378861, 0.01539487, 0.76724275, 0.29440457]),\n",
       "  array([0.01553299, 0.20973171, 0.39016441, 0.20973171, 0.04355649,\n",
       "         0.04355649, 0.01553299, 0.76627016, 0.29380221]),\n",
       "  array([0.01565783, 0.21018976, 0.39078906, 0.21018976, 0.04334811,\n",
       "         0.04334811, 0.01565783, 0.76539535, 0.29326062]),\n",
       "  array([0.01577061, 0.21060244, 0.39135168, 0.21060244, 0.043161  ,\n",
       "         0.043161  , 0.01577061, 0.76460845, 0.29277362]),\n",
       "  array([0.01587246, 0.21097419, 0.39185839, 0.21097419, 0.04299294,\n",
       "         0.04299294, 0.01587246, 0.76390059, 0.29233566]),\n",
       "  array([0.0159644 , 0.21130904, 0.3923147 , 0.21130904, 0.04284198,\n",
       "         0.04284198, 0.0159644 , 0.7632638 , 0.29194177]),\n",
       "  array([0.01604738, 0.21161064, 0.39272561, 0.21161064, 0.04270633,\n",
       "         0.04270633, 0.01604738, 0.76269091, 0.29158751]),\n",
       "  array([0.01612224, 0.21188226, 0.39309561, 0.21188226, 0.04258443,\n",
       "         0.04258443, 0.01612224, 0.76217549, 0.29126885]),\n",
       "  array([0.01618976, 0.21212687, 0.39342876, 0.21212687, 0.04247487,\n",
       "         0.04247487, 0.01618976, 0.76171176, 0.29098221]),\n",
       "  array([0.01625066, 0.21234714, 0.39372872, 0.21234714, 0.04237639,\n",
       "         0.04237639, 0.01625066, 0.76129453, 0.29072435]),\n",
       "  array([0.01630556, 0.21254548, 0.39399878, 0.21254548, 0.04228786,\n",
       "         0.04228786, 0.01630556, 0.76091912, 0.29049238]),\n",
       "  array([0.01635505, 0.21272407, 0.39424191, 0.21272407, 0.04220825,\n",
       "         0.04220825, 0.01635505, 0.76058133, 0.29028368]),\n",
       "  array([0.01639965, 0.21288486, 0.39446079, 0.21288486, 0.04213667,\n",
       "         0.04213667, 0.01639965, 0.76027738, 0.29009592]),\n",
       "  array([0.01643985, 0.21302962, 0.39465784, 0.21302962, 0.04207231,\n",
       "         0.04207231, 0.01643985, 0.76000388, 0.28992698]),\n",
       "  array([0.01647607, 0.21315995, 0.39483522, 0.21315995, 0.04201442,\n",
       "         0.04201442, 0.01647607, 0.75975777, 0.28977498]),\n",
       "  array([0.0165087 , 0.21327728, 0.3949949 , 0.21327728, 0.04196235,\n",
       "         0.04196235, 0.0165087 , 0.75953631, 0.28963822]),\n",
       "  array([0.0165381 , 0.2133829 , 0.39513864, 0.2133829 , 0.04191552,\n",
       "         0.04191552, 0.0165381 , 0.75933702, 0.28951515]),\n",
       "  array([0.01656458, 0.21347799, 0.39526803, 0.21347799, 0.04187339,\n",
       "         0.04187339, 0.01656458, 0.75915768, 0.28940442]),\n",
       "  array([0.01658843, 0.21356358, 0.3953845 , 0.21356358, 0.04183549,\n",
       "         0.04183549, 0.01658843, 0.75899629, 0.28930478]),\n",
       "  array([0.01660991, 0.21364063, 0.39548933, 0.21364063, 0.0418014 ,\n",
       "         0.0418014 , 0.01660991, 0.75885106, 0.28921512]),\n",
       "  array([0.01662925, 0.21370999, 0.39558369, 0.21370999, 0.04177073,\n",
       "         0.04177073, 0.01662925, 0.75872036, 0.28913443]),\n",
       "  array([0.01664667, 0.21377242, 0.39566863, 0.21377242, 0.04174314,\n",
       "         0.04174314, 0.01664667, 0.75860274, 0.28906183]),\n",
       "  array([0.01666235, 0.21382862, 0.39574508, 0.21382862, 0.04171831,\n",
       "         0.04171831, 0.01666235, 0.75849689, 0.28899649]),\n",
       "  array([0.01667647, 0.2138792 , 0.39581389, 0.2138792 , 0.04169597,\n",
       "         0.04169597, 0.01667647, 0.75840163, 0.28893769]),\n",
       "  array([0.01668919, 0.21392473, 0.39587583, 0.21392473, 0.04167587,\n",
       "         0.04167587, 0.01668919, 0.75831591, 0.28888478]),\n",
       "  array([0.01670064, 0.21396571, 0.39593158, 0.21396571, 0.04165779,\n",
       "         0.04165779, 0.01670064, 0.75823876, 0.28883716]),\n",
       "  array([0.01671094, 0.21400259, 0.39598175, 0.21400259, 0.04164151,\n",
       "         0.04164151, 0.01671094, 0.75816932, 0.28879431]),\n",
       "  array([0.01672022, 0.21403579, 0.39602691, 0.21403579, 0.04162687,\n",
       "         0.04162687, 0.01672022, 0.75810684, 0.28875574]),\n",
       "  array([0.01672857, 0.21406567, 0.39606756, 0.21406567, 0.04161369,\n",
       "         0.04161369, 0.01672857, 0.7580506 , 0.28872104]),\n",
       "  array([0.01673609, 0.21409257, 0.39610414, 0.21409257, 0.04160184,\n",
       "         0.04160184, 0.01673609, 0.75799999, 0.2886898 ]),\n",
       "  array([0.01674286, 0.21411678, 0.39613707, 0.21411678, 0.04159117,\n",
       "         0.04159117, 0.01674286, 0.75795445, 0.2886617 ]),\n",
       "  array([0.01674895, 0.21413856, 0.3961667 , 0.21413856, 0.04158157,\n",
       "         0.04158157, 0.01674895, 0.75791345, 0.2886364 ]),\n",
       "  array([0.01675444, 0.21415817, 0.39619338, 0.21415817, 0.04157292,\n",
       "         0.04157292, 0.01675444, 0.75787656, 0.28861363]),\n",
       "  array([0.01675938, 0.21417582, 0.39621738, 0.21417582, 0.04156515,\n",
       "         0.04156515, 0.01675938, 0.75784336, 0.28859314]),\n",
       "  array([0.01676382, 0.21419171, 0.39623899, 0.21419171, 0.04155815,\n",
       "         0.04155815, 0.01676382, 0.75781348, 0.28857471]),\n",
       "  array([0.01676782, 0.21420601, 0.39625843, 0.21420601, 0.04155185,\n",
       "         0.04155185, 0.01676782, 0.75778659, 0.28855811]),\n",
       "  array([0.01677142, 0.21421888, 0.39627594, 0.21421888, 0.04154619,\n",
       "         0.04154619, 0.01677142, 0.75776239, 0.28854318]),\n",
       "  array([0.01677466, 0.21423046, 0.39629169, 0.21423046, 0.04154109,\n",
       "         0.04154109, 0.01677466, 0.7577406 , 0.28852973]),\n",
       "  array([0.01677758, 0.21424088, 0.39630587, 0.21424088, 0.0415365 ,\n",
       "         0.0415365 , 0.01677758, 0.757721  , 0.28851764]),\n",
       "  array([0.0167802 , 0.21425026, 0.39631863, 0.21425026, 0.04153237,\n",
       "         0.04153237, 0.0167802 , 0.75770336, 0.28850675]),\n",
       "  array([0.01678257, 0.21425871, 0.39633011, 0.21425871, 0.04152865,\n",
       "         0.04152865, 0.01678257, 0.75768748, 0.28849695]),\n",
       "  array([0.01678469, 0.2142663 , 0.39634044, 0.2142663 , 0.0415253 ,\n",
       "         0.0415253 , 0.01678469, 0.75767319, 0.28848814]),\n",
       "  array([0.01678661, 0.21427314, 0.39634975, 0.21427314, 0.04152229,\n",
       "         0.04152229, 0.01678661, 0.75766033, 0.2884802 ]),\n",
       "  array([0.01678833, 0.2142793 , 0.39635812, 0.2142793 , 0.04151958,\n",
       "         0.04151958, 0.01678833, 0.75764875, 0.28847306]),\n",
       "  array([0.01678988, 0.21428484, 0.39636565, 0.21428484, 0.04151714,\n",
       "         0.04151714, 0.01678988, 0.75763834, 0.28846663]),\n",
       "  array([0.01679128, 0.21428983, 0.39637243, 0.21428983, 0.04151495,\n",
       "         0.04151495, 0.01679128, 0.75762896, 0.28846084]),\n",
       "  array([0.01679253, 0.21429431, 0.39637854, 0.21429431, 0.04151297,\n",
       "         0.04151297, 0.01679253, 0.75762052, 0.28845564]),\n",
       "  array([0.01679366, 0.21429835, 0.39638403, 0.21429835, 0.0415112 ,\n",
       "         0.0415112 , 0.01679366, 0.75761293, 0.28845095]),\n",
       "  array([0.01679468, 0.21430199, 0.39638898, 0.21430199, 0.0415096 ,\n",
       "         0.0415096 , 0.01679468, 0.75760609, 0.28844673]),\n",
       "  array([0.0167956 , 0.21430526, 0.39639342, 0.21430526, 0.04150816,\n",
       "         0.04150816, 0.0167956 , 0.75759994, 0.28844294]),\n",
       "  array([0.01679642, 0.2143082 , 0.39639743, 0.2143082 , 0.04150686,\n",
       "         0.04150686, 0.01679642, 0.75759441, 0.28843952]),\n",
       "  array([0.01679716, 0.21431085, 0.39640103, 0.21431085, 0.04150569,\n",
       "         0.04150569, 0.01679716, 0.75758942, 0.28843645]),\n",
       "  array([0.01679783, 0.21431324, 0.39640428, 0.21431324, 0.04150464,\n",
       "         0.04150464, 0.01679783, 0.75758494, 0.28843368]),\n",
       "  array([0.01679843, 0.21431539, 0.3964072 , 0.21431539, 0.0415037 ,\n",
       "         0.0415037 , 0.01679843, 0.7575809 , 0.28843119]),\n",
       "  array([0.01679897, 0.21431732, 0.39640982, 0.21431732, 0.04150285,\n",
       "         0.04150285, 0.01679897, 0.75757727, 0.28842895]),\n",
       "  array([0.01679946, 0.21431906, 0.39641219, 0.21431906, 0.04150208,\n",
       "         0.04150208, 0.01679946, 0.757574  , 0.28842693])])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent0(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Good, so \n",
    "\n",
    "You want to see \n",
    "\n",
    "\n",
    "\n",
    "Start off with what you can observe and one you want to predict. \n",
    "\n",
    "\n",
    "\n",
    "As you move through one you see that it co moveswith another. like rolling stick. \n",
    "\n",
    "Can you not try to draw a line next to it connecting the two. \n",
    "\n",
    "It might be better to visualize them next to each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources and References:\n",
    "\n",
    "- Introduction to Statistical Learning\n",
    "- Elements of Statistical Learning\n",
    "- Introduction to Econometrics by Wooldridge\n",
    "- Khan Academy\n",
    "- Ritvik Khakhar's Video (Seriously check it out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
